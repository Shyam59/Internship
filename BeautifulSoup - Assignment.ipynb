{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00c7d854",
   "metadata": {},
   "source": [
    "1) Write a python program to display IMDB’s Top rated 100 Indian movies’ data https://www.imdb.com/list/ls056092300/ (i.e. name, rating, year ofrelease) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a56eed33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Title: 403 Forbidden\n",
      "Found 0 movies\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.imdb.com/list/ls056092300/'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "print(\"Page Title:\", soup.title.string)\n",
    "\n",
    "movies = soup.find_all('div', class_='lister-item mode-detail')\n",
    "\n",
    "print(f\"Found {len(movies)} movies\")\n",
    "\n",
    "movie_names = []\n",
    "movie_ratings = []\n",
    "movie_years = []\n",
    "\n",
    "for movie in movies:\n",
    "    \n",
    "    title = movie.find('h3', class_='lister-item-header').a.text.strip()\n",
    "    movie_names.append(title)\n",
    "    \n",
    "    rating_tag = movie.find('div', class_='ipl-rating-star small')\n",
    "    rating = rating_tag.span.text if rating_tag else 'N/A'\n",
    "    movie_ratings.append(rating)\n",
    "    \n",
    "    year = movie.find('span', class_='lister-item-year').text.strip()\n",
    "    movie_years.append(year)\n",
    "    \n",
    "    data = {\n",
    "        'Movie Name': movie_names,\n",
    "        'Rating': movie_ratings,\n",
    "        'Release Year': movie_years\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879c4e76",
   "metadata": {},
   "source": [
    "2) Write a python program to scrape details of all the posts from https://www.patreon.com/coreyms .Scrape the heading, date, content and the likes for the video from the link for the youtube video from the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95c0c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.patreon.com/coreyms'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "posts = soup.find_all('div', class_='post-class')\n",
    "\n",
    "for post in posts:\n",
    "   \n",
    "    heading = post.find('h2', class_='heading-class').text.strip()\n",
    "\n",
    "    date = post.find('span', class_='date-class').text.strip()\n",
    "\n",
    "    content = post.find('div', class_='content-class').text.strip()\n",
    "\n",
    "    video_link = post.find('a', class_='youtube-link-class')['href']\n",
    "\n",
    "    likes = post.find('span', class_='likes-class').text.strip()\n",
    "\n",
    "    print(f'Heading: {heading}')\n",
    "    print(f'Date: {date}')\n",
    "    print(f'Content: {content}')\n",
    "    print(f'YouTube Video Link: {video_link}')\n",
    "    print(f'Likes: {likes}')\n",
    "    print('-' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614f1b39",
   "metadata": {},
   "source": [
    "3) Write a python program to scrape house details from mentioned URL. It should include house title, location, area, EMI and price from https://www.nobroker.in/ .Enter three localities which are Indira Nagar, Jayanagar, Rajaji Nagar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db1efd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locality: Indira Nagar\n",
      "No listings found for Indira Nagar\n",
      "Locality: Jayanagar\n",
      "No listings found for Jayanagar\n",
      "Locality: Rajaji Nagar\n",
      "No listings found for Rajaji Nagar\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "localities = ['Indira Nagar', 'Jayanagar', 'Rajaji Nagar']\n",
    "\n",
    "def scrape_house_details(locality):\n",
    "    url = f'https://www.nobroker.in/property/search/{locality}?searchParam={{}}'\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    listings = soup.find_all('div', class_='card')\n",
    "\n",
    "    if not listings:\n",
    "        print(f'No listings found for {locality}')\n",
    "        return\n",
    "\n",
    "    for listing in listings:\n",
    "        try:\n",
    "            title = listing.find('h2', class_='title').text.strip()\n",
    "            location = listing.find('span', class_='location').text.strip()\n",
    "            area = listing.find('div', class_='area').text.strip()\n",
    "            emi = listing.find('div', class_='emi').text.strip()\n",
    "            price = listing.find('div', class_='price').text.strip()\n",
    "            \n",
    "            print(f'Title: {title}')\n",
    "            print(f'Location: {location}')\n",
    "            print(f'Area: {area}')\n",
    "            print(f'EMI: {emi}')\n",
    "            print(f'Price: {price}')\n",
    "            print('-' * 40)\n",
    "        \n",
    "        except AttributeError:\n",
    "            print('Error extracting one or more details.')\n",
    "            continue\n",
    "            \n",
    "for locality in localities:\n",
    "    print(f'Locality: {locality}')\n",
    "    scrape_house_details(locality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b1e76a",
   "metadata": {},
   "source": [
    "4) Write a python program to scrape first 10 product details which include product name , price , Image URL from https://www.bewakoof.com/bestseller?sort=popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa7746fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.bewakoof.com/bestseller?sort=popular\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "products = soup.find_all('div', class_='bestseller-item', limit=10)\n",
    "\n",
    "for product in products:\n",
    "    try:\n",
    "        name = product.find('a', class_='product-name').text.strip()\n",
    "        \n",
    "        price = product.find('span', class_='discountedPrice').text.strip()\n",
    "        \n",
    "        img_tag = product.find('img', class_='product-image')\n",
    "        img_url = img_tag['src'] if img_tag else 'No image URL found'\n",
    "        \n",
    "        print(f'Product Name: {name}')\n",
    "        print(f'Price: {price}')\n",
    "        print(f'Image URL: {img_url}')\n",
    "        print('-' * 40)\n",
    "        \n",
    "    except AttributeError as e:\n",
    "        print(f'Error extracting details: {e}')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904f8487",
   "metadata": {},
   "source": [
    "5) Please visit https://www.cnbc.com/world/?region=world and scrap- a)   headings b)  date c) News link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20867bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "articles = soup.find_all('article', class_='LatestNews-article')\n",
    "\n",
    "for article in articles:\n",
    "    try:\n",
    "        headline = article.find('a', class_='LatestNews-headline').text.strip()\n",
    "\n",
    "        date = article.find('time', class_='LatestNews-time').text.strip() if article.find('time', class_='LatestNews-time') else 'No date available'\n",
    "\n",
    "        link = article.find('a', class_='LatestNews-headline')['href']\n",
    "        full_link = f\"https://www.cnbc.com{link}\"\n",
    "\n",
    "        print(f'Headline: {headline}')\n",
    "        print(f'Date: {date}')\n",
    "        print(f'Link: {full_link}')\n",
    "        print('-' * 40)\n",
    "    except AttributeError as e:\n",
    "        print(f'Error extracting details: {e}')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e175a6d0",
   "metadata": {},
   "source": [
    "6) Please visit  https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloadedarticles/  and scrap- a) Paper title b) date c) Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "664f8800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "articles = soup.find_all('div', class_='article-details')\n",
    "\n",
    "for article in articles:\n",
    "    try:\n",
    "        title = article.find('h3', class_='article-title').text.strip()\n",
    "        \n",
    "        date = article.find('span', class_='date').text.strip()\n",
    "        \n",
    "        authors = article.find('div', class_='authors').text.strip()\n",
    "        \n",
    "        print(f'Paper Title: {title}')\n",
    "        \n",
    "        print(f'Date: {date}')\n",
    "        \n",
    "        print(f'Authors: {authors}')\n",
    "        \n",
    "        print('-' * 40)\n",
    "        \n",
    "    except AttributeError as e:\n",
    "        print(f'Error extracting details: {e}')\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
